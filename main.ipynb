{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# URL Security Classification using Q-Learning\n",
        "\n",
        "This notebook provides an interactive way to run the complete pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all necessary modules\n",
        "import sys\n",
        "from data_preparation import DataPreparator\n",
        "from environment import URLSecurityEnvironment\n",
        "from q_learning import QLearningAgent\n",
        "from baseline import RuleBasedBaseline\n",
        "from training import train_q_learning_agent\n",
        "from evaluation import (evaluate_agent, evaluate_baseline, generate_all_visualizations, \n",
        "                       save_evaluation_results)\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset from dataset/csic_database.csv...\n",
            "Loaded 61065 samples\n",
            "Benign samples: 36000, Attack samples: 25065\n",
            "Extracting features from URLs...\n",
            "Feature extraction complete. Shape: (61065, 6)\n",
            "Discretization fitted using binning method\n",
            "Total number of discrete states: 162\n",
            "Data split:\n",
            "  Training: 42745 samples\n",
            "  Validation: 9160 samples\n",
            "  Test: 9160 samples\n"
          ]
        }
      ],
      "source": [
        "# Initialize data preparator\n",
        "preparator = DataPreparator(dataset_path='dataset/csic_database.csv')\n",
        "\n",
        "# Load data\n",
        "data = preparator.load_data()\n",
        "\n",
        "# Extract features\n",
        "features = preparator.extract_features(data)\n",
        "\n",
        "# Fit discretization\n",
        "preparator.fit_discretization(features.drop('label', axis=1), method='binning')\n",
        "num_states = preparator.get_num_states()\n",
        "print(f\"Total number of discrete states: {num_states}\")\n",
        "\n",
        "# Split data\n",
        "train_data, val_data, test_data = preparator.split_data(features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Baseline Approach\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tuned K to 0 with accuracy 0.6366\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0, np.float64(0.6365720524017467))"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize baseline\n",
        "baseline = RuleBasedBaseline(K=2)\n",
        "\n",
        "# Tune threshold on validation set\n",
        "baseline.tune_threshold(val_data.drop('label', axis=1), val_data['label'].values)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. RL Environment Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment created with episode length: 1000\n"
          ]
        }
      ],
      "source": [
        "# Create environment\n",
        "env = URLSecurityEnvironment(train_data, preparator, episode_length=1000)\n",
        "print(f\"Environment created with episode length: {env.episode_length}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Q-Learning Agent Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q-learning agent initialized with 162 states\n"
          ]
        }
      ],
      "source": [
        "# Initialize Q-learning agent\n",
        "agent = QLearningAgent(\n",
        "    num_states=num_states,\n",
        "    num_actions=2,\n",
        "    alpha=0.1,\n",
        "    gamma=0.90,\n",
        "    epsilon_start=1.0,\n",
        "    epsilon_min=0.05,\n",
        "    epsilon_decay=0.98\n",
        ")\n",
        "print(f\"Q-learning agent initialized with {num_states} states\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training for 15 episodes...\n",
            "Initial epsilon: 1.0000\n",
            "Episode 1/15 - Reward: -2709.50, Avg Reward (last 10): -2709.50, Epsilon: 0.9800\n",
            "Episode 2/15 - Reward: -2411.50, Avg Reward (last 10): -2411.50, Epsilon: 0.9604\n",
            "Episode 3/15 - Reward: -2404.00, Avg Reward (last 10): -2404.00, Epsilon: 0.9412\n",
            "Episode 4/15 - Reward: -2591.00, Avg Reward (last 10): -2591.00, Epsilon: 0.9224\n",
            "Episode 5/15 - Reward: -2574.00, Avg Reward (last 10): -2574.00, Epsilon: 0.9039\n",
            "Episode 6/15 - Reward: -2633.50, Avg Reward (last 10): -2633.50, Epsilon: 0.8858\n",
            "Episode 7/15 - Reward: -2569.00, Avg Reward (last 10): -2569.00, Epsilon: 0.8681\n",
            "Episode 8/15 - Reward: -2762.50, Avg Reward (last 10): -2762.50, Epsilon: 0.8508\n",
            "Episode 9/15 - Reward: -2358.00, Avg Reward (last 10): -2358.00, Epsilon: 0.8337\n",
            "Episode 10/15 - Reward: -2451.50, Avg Reward (last 10): -2546.45, Epsilon: 0.8171\n",
            "Episode 11/15 - Reward: -2497.50, Avg Reward (last 10): -2525.25, Epsilon: 0.8007\n",
            "Episode 12/15 - Reward: -2218.50, Avg Reward (last 10): -2505.95, Epsilon: 0.7847\n",
            "Episode 13/15 - Reward: -2437.00, Avg Reward (last 10): -2509.25, Epsilon: 0.7690\n",
            "Episode 14/15 - Reward: -2311.00, Avg Reward (last 10): -2481.25, Epsilon: 0.7536\n",
            "Episode 15/15 - Reward: -2255.00, Avg Reward (last 10): -2449.35, Epsilon: 0.7386\n",
            "\n",
            "Training complete!\n",
            "Average reward per episode: -2478.90\n",
            "Final epsilon: 0.7386\n",
            "Q-table saved to q_table.npy\n",
            "Agent saved to q_learning_agent.pkl\n"
          ]
        }
      ],
      "source": [
        "# Train the agent\n",
        "num_episodes = 15\n",
        "training_stats = train_q_learning_agent(env, agent, num_episodes=num_episodes, verbose=True)\n",
        "\n",
        "# Save Q-table\n",
        "agent.save_q_table('q_table.npy')\n",
        "agent.save_agent('q_learning_agent.pkl')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "RL Agent Evaluation Results\n",
            "==================================================\n",
            "Accuracy:  0.6810\n",
            "Precision: 0.6852\n",
            "Recall:    0.4122\n",
            "F1-Score:  0.5148\n",
            "Avg Reward: -1.7245\n",
            "\n",
            "Confusion Matrix:\n",
            "              Predicted\n",
            "              ALLOW  BLOCK\n",
            "Actual Benign  4688    712\n",
            "Actual Attack  2210   1550\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "Baseline Evaluation Results\n",
            "==================================================\n",
            "Accuracy:  0.6443\n",
            "Precision: 0.6303\n",
            "Recall:    0.3229\n",
            "F1-Score:  0.4270\n",
            "Avg Reward: -2.0913\n",
            "\n",
            "Confusion Matrix:\n",
            "              Predicted\n",
            "              ALLOW  BLOCK\n",
            "Actual Benign  4688    712\n",
            "Actual Attack  2546   1214\n",
            "==================================================\n",
            "Learning curve saved to plots\\learning_curve.png\n",
            "Confusion matrix saved to plots\\confusion_matrix_rl.png\n",
            "Confusion matrix saved to plots\\confusion_matrix_baseline.png\n",
            "Q-table has 162 states, showing first 100 for visualization\n",
            "Q-table heatmap saved to plots\\q_table_heatmap.png\n",
            "Evaluation results saved to evaluation_results.json\n"
          ]
        }
      ],
      "source": [
        "# Evaluate RL agent\n",
        "rl_results = evaluate_agent(agent, env, preparator, test_data, verbose=True)\n",
        "\n",
        "# Evaluate baseline\n",
        "baseline_results = evaluate_baseline(baseline, test_data, verbose=True)\n",
        "\n",
        "# Generate visualizations\n",
        "generate_all_visualizations(agent, rl_results, baseline_results, output_dir='plots')\n",
        "\n",
        "# Save evaluation results\n",
        "save_evaluation_results(rl_results, baseline_results, 'evaluation_results.json')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Generate Report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Report generated and saved to report.md\n",
            "Report generated and saved to report.md\n"
          ]
        }
      ],
      "source": [
        "# Import report generation function from main\n",
        "from main import generate_report\n",
        "\n",
        "# Generate report\n",
        "generate_report(agent, preparator, rl_results, baseline_results, training_stats)\n",
        "print(\"Report generated and saved to report.md\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "The complete pipeline has been executed. Check the output files:\n",
        "- `q_table.npy`: Saved Q-table\n",
        "- `q_learning_agent.pkl`: Saved agent\n",
        "- `evaluation_results.json`: Evaluation metrics\n",
        "- `plots/`: Visualizations\n",
        "- `report.md`: Comprehensive report\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
